{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "The code presented in this notebook is a modified version from a repository made available by Hemker (2018). Unfortunately, the author has since deleted the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T05:22:04.237721Z",
     "start_time": "2020-01-09T05:22:04.207795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import re\n",
    "import gensim\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import preprocess\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def get_corpus(corpus_, path=''):\n",
    "    \"\"\"Loads pre-trained word2vec model from src/ directory and\n",
    "    returns a gensim word2vec object\"\"\"\n",
    "    if corpus_ == 'google':\n",
    "        return KeyedVectors.load_word2vec_format(path + 'GoogleNews-vectors-negative300.bin',\n",
    "                                                 binary=True)\n",
    "    if corpus_=='glove':\n",
    "        return KeyedVectors.load_word2vec_format(path + 'glove.42B.300d.txt',\n",
    "                                                 binary=False)\n",
    "    if corpus_=='glove25':\n",
    "        return KeyedVectors.load_word2vec_format(path + 'glove.twitter.27B.25d.txt',\n",
    "                                                 binary=False)\n",
    "    if corpus_=='fasttext':\n",
    "        return KeyedVectors.load_word2vec_format(path + 'crawl-300d-2M.vec',\n",
    "                                                 binary=False,\n",
    "                                                 encoding='UTF-8')\n",
    "\n",
    "class Augment():\n",
    "\n",
    "    def __init__(self,\n",
    "                 source_path,\n",
    "                 target_path,\n",
    "                 corpus_='none',\n",
    "                 valid_tags=['NN'],\n",
    "                 threshold=0.75,\n",
    "                 x_col='tweet',\n",
    "                 y_col='class',\n",
    "                 path='',\n",
    "                 model=None):\n",
    "        \"\"\"\n",
    "        Constructor Arguments\n",
    "        source_path (string): csv file that is meant to be augmented\n",
    "        corpus_ (string): Word corpus that the similarity model should take in\n",
    "            valid args: ['none', 'glove', 'fasttext', 'google']\n",
    "        x_col (string): column name in csv from samples\n",
    "        y_col (string): column name in csv for labels\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            self.model = get_corpus(corpus_, path)  # Load model\n",
    "            print('Loaded corpus: ', corpus_)\n",
    "        else:\n",
    "            self.model = model\n",
    "           \n",
    "        self.x_col=x_col\n",
    "        self.y_col=y_col\n",
    "        self.df=pd.read_csv(source_path)\n",
    "        self.augmented=pd.DataFrame(columns=[x_col, y_col])\n",
    "        self.valid_tags = valid_tags\n",
    "        self.threshold_ = threshold\n",
    "        \n",
    "        # Store a mapping from original data point to augmented ones\n",
    "        try:\n",
    "            self.aug_idxs = pickle.load(open(\"data/augmentation_map.pickle\", \"rb\"))\n",
    "        except EnvironmentError:\n",
    "            self.aug_idxs = dict()\n",
    "\n",
    "        # Go through each row in dataframe\n",
    "        for idx, row in self.df.iterrows():\n",
    "           \n",
    "            x = preprocess(row[self.x_col]).split()  # Preprocess input\n",
    "            y = row[self.y_col]\n",
    "            aug_temp = self.threshold(x)\n",
    "            idx2aug = [] # mapping for this data point\n",
    "\n",
    "            for elem in aug_temp:\n",
    "                new_idx = self.augmented.shape[0] # index in the augmented dataset\n",
    "                self.augmented.loc[new_idx] = [elem, y]\n",
    "                idx2aug.append(new_idx)\n",
    "               \n",
    "            self.aug_idxs[idx] = idx2aug\n",
    "            \n",
    "            if (idx+1) % 10 == 0:\n",
    "                print(\"{} rows successfully augmented.\".format(idx+1))\n",
    "                self.augmented.to_csv(target_path, encoding='utf-8')\n",
    "                \n",
    "                with open(\"data/augmentation_map.pickle\", \"wb\") as f:\n",
    "                    pickle.dump(self.aug_idxs, f, pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "            if idx == 5: break\n",
    "       \n",
    "        print(\"Augmentation complete.\")\n",
    "        self.augmented.to_csv(target_path, encoding='utf-8')\n",
    "        with open(\"data/augmentation_map.pickle\", \"wb\") as f:\n",
    "            pickle.dump(self.aug_idxs, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "\n",
    "    def create_augmented_samples(self, dict, n, x):\n",
    "        \"\"\"Function receives a dictionary which contains the acceptable substitutions for each\n",
    "           word in x.\"\"\"\n",
    "        aug_tweets = [' '.join(x)]  # Save original tweet\n",
    "       \n",
    "        # For each possible substitution\n",
    "        for i in range(n):  \n",
    "           \n",
    "            # copy the original tweet\n",
    "            single_augment = x.copy()  \n",
    "           \n",
    "            # For each word in the tweet\n",
    "            for idx, word in enumerate(single_augment):\n",
    "               \n",
    "                # If the word can be replaced and we haven't used all the possible replacements before\n",
    "                if word in dict.keys() and len(dict[word]) >= i+1:\n",
    "                   \n",
    "                    # Replace that word\n",
    "                    single_augment[idx] = dict[word][i]\n",
    "                   \n",
    "            # Join the words into a sentence\n",
    "            single_augment = ' '.join(single_augment)\n",
    "           \n",
    "            # Save the augmented tweet\n",
    "            aug_tweets.append(single_augment)\n",
    "           \n",
    "        return aug_tweets\n",
    "\n",
    "\n",
    "    def threshold(self, x):\n",
    "       \n",
    "        # Create a dictionary that will save the possible replacements for each word\n",
    "        dict = {}\n",
    "        n = 0\n",
    "       \n",
    "        # Generate POS tags for the words in sentence x\n",
    "        tags = pos_tag(x)  \n",
    "       \n",
    "        for idx, word in enumerate(x):  # For each word in x\n",
    "           \n",
    "            # Check if word is part of the vocabulary\n",
    "            if word in self.model.vocab:  \n",
    "               \n",
    "                #get words with highest cosine similarity\n",
    "                replacements = self.model.most_similar(positive=word, topn=5)\n",
    "               \n",
    "                #keep only words that pass the threshold\n",
    "                replacements = [replacements[i][0] for i in range(5) if replacements[i][1] > self.threshold_]\n",
    "               \n",
    "                #check for POS tag equality, dismiss if unequal\n",
    "                replacements = [elem for elem in replacements if pos_tag([elem.lower()])[0][1] == tags[idx][1]]\n",
    "               \n",
    "                #update dictionary with possible replacements for key word\n",
    "                dict.update({word:replacements}) if len(replacements) > 0 else dict\n",
    "                n = max(len(replacements), n) #update largest number of replacements\n",
    "       \n",
    "        return self.create_augmented_samples(dict, n, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T05:22:04.972200Z",
     "start_time": "2020-01-09T05:22:04.967213Z"
    }
   },
   "outputs": [],
   "source": [
    "source_path = 'data/labeled_data.csv'\n",
    "target_path = 'data/augmented_data.csv'\n",
    "corpus_='glove'\n",
    "path=\"./glove/\"  # Corpus path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T05:33:13.498032Z",
     "start_time": "2020-01-09T05:22:05.469459Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_corpus(corpus_, path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-09T05:35:26.149311Z",
     "start_time": "2020-01-09T05:35:05.687188Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmentation complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Augment at 0x25242d7a588>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May take several hours.\n",
    "Augment(source_path=source_path, target_path=target_path, corpus_=corpus_, path=path, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "231px",
    "width": "222px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 296,
   "position": {
    "height": "316.8px",
    "left": "1167px",
    "right": "20px",
    "top": "124px",
    "width": "326.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
