{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "\n",
    "The code presented in this notebook is a modified version from a repository made available by Hemker (2018). Unfortunately, the author has since deleted the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:25:05.224104Z",
     "start_time": "2020-01-05T09:24:54.678403Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import pos_tag\n",
    "import gensim\n",
    "import re\n",
    "from nltk.corpus import wordnet as wn\n",
    "import os\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "def get_corpus(corpus_, path=''):\n",
    "    \"\"\"Loads pre-trained word2vec model from src/ directory and\n",
    "    returns a gensim word2vec object\"\"\"\n",
    "    if corpus_ == 'google':\n",
    "        return KeyedVectors.load_word2vec_format(path + 'GoogleNews-vectors-negative300.bin',\n",
    "                                                 binary=True)\n",
    "    if corpus_=='glove':\n",
    "        return KeyedVectors.load_word2vec_format(path + 'glove.42B.300d.txt',\n",
    "                                                 binary=False)\n",
    "    if corpus_=='glove25':\n",
    "        return KeyedVectors.load_word2vec_format(path + 'glove.twitter.27B.25d.txt',\n",
    "                                                 binary=False)\n",
    "    if corpus_=='fasttext':\n",
    "        return KeyedVectors.load_word2vec_format(path + 'crawl-300d-2M.vec',\n",
    "                                                 binary=False,\n",
    "                                                 encoding='UTF-8')\n",
    "\n",
    "class Augment():\n",
    "\n",
    "    def __init__(self,\n",
    "                 source_path,\n",
    "                 target_path,\n",
    "                 corpus_='none',\n",
    "                 valid_tags=['NN'],\n",
    "                 threshold=0.75,\n",
    "                 x_col='tweet',\n",
    "                 y_col='class',\n",
    "                 path='',\n",
    "                 model=None):\n",
    "        \"\"\"\n",
    "        Constructor Arguments\n",
    "        source_path (string): csv file that is meant to be augmented\n",
    "        corpus_ (string): Word corpus that the similarity model should take in\n",
    "            valid args: ['none', 'glove', 'fasttext', 'google']\n",
    "        x_col (string): column name in csv from samples\n",
    "        y_col (string): column name in csv for labels\n",
    "        \"\"\"\n",
    "        if model is None:\n",
    "            self.model = get_corpus(corpus_, path)  # Load model\n",
    "            print('Loaded corpus: ', corpus_)\n",
    "        else:\n",
    "            self.model = model\n",
    "            \n",
    "        self.x_col=x_col\n",
    "        self.y_col=y_col\n",
    "        self.df=pd.read_csv(source_path)\n",
    "        self.augmented=pd.DataFrame(columns=[x_col, y_col])\n",
    "        self.valid_tags = valid_tags\n",
    "        self.threshold_ = threshold\n",
    "\n",
    "        # Go through each row in dataframe\n",
    "        for idx, row in self.df.iterrows():\n",
    "            \n",
    "            x = self.preprocess(row[self.x_col])  # Preprocess input\n",
    "            y = row[self.y_col]\n",
    "            aug_temp = self.threshold(x)\n",
    "\n",
    "            for elem in aug_temp:\n",
    "                self.augmented.loc[self.augmented.shape[0]] = [elem, y]\n",
    "                \n",
    "            if (idx+1) % 200 == 0:\n",
    "                print(\"{} rows successfully augmented.\".format(idx+1))\n",
    "                self.augmented.to_csv(target_path, encoding='utf-8')\n",
    "        \n",
    "        print(\"Augmentation complete.\")\n",
    "        self.augmented.to_csv(target_path, encoding='utf-8')\n",
    "\n",
    "\n",
    "    def preprocess(self, x):\n",
    "        \"\"\"Minimalistic preprocessing of the input\"\"\"\n",
    "        x = re.sub(\"[^a-zA-Z ]+\", \"\", x)\n",
    "        x = x.split()\n",
    "        return x\n",
    "\n",
    "    def create_augmented_samples(self, dict, n, x):\n",
    "        \"\"\"Function receives a dictionary which contains the acceptable substitutions for each \n",
    "           word in x.\"\"\"\n",
    "        aug_tweets = [' '.join(x)]  # Save original tweet\n",
    "        \n",
    "        # For each possible substitution\n",
    "        for i in range(n):  \n",
    "            \n",
    "            # copy the original tweet\n",
    "            single_augment = x.copy()  \n",
    "            \n",
    "            # For each word in the tweet\n",
    "            for idx, word in enumerate(single_augment):\n",
    "                \n",
    "                # If the word can be replaced and we haven't used all the possible replacements before\n",
    "                if word in dict.keys() and len(dict[word]) >= i+1:\n",
    "                    \n",
    "                    # Replace that word\n",
    "                    single_augment[idx] = dict[word][i]\n",
    "                    \n",
    "            # Join the words into a sentence\n",
    "            single_augment = ' '.join(single_augment)\n",
    "            \n",
    "            # Save the augmented tweet\n",
    "            aug_tweets.append(single_augment)\n",
    "            \n",
    "        return aug_tweets\n",
    "\n",
    "\n",
    "    def threshold(self, x):\n",
    "        \n",
    "        # Create a dictionary that will save the possible replacements for each word\n",
    "        dict = {}\n",
    "        n = 0\n",
    "        \n",
    "        # Generate POS tags for the words in sentence x\n",
    "        tags = pos_tag(x)  \n",
    "        \n",
    "        for idx, word in enumerate(x):  # For each word in x\n",
    "            \n",
    "            # Check if word is part of the vocabulary\n",
    "            if word in self.model.vocab:  \n",
    "                \n",
    "                #get words with highest cosine similarity\n",
    "                replacements = self.model.most_similar(positive=word, topn=5)\n",
    "                \n",
    "                #keep only words that pass the threshold\n",
    "                replacements = [replacements[i][0] for i in range(5) if replacements[i][1] > self.threshold_]\n",
    "                \n",
    "                #check for POS tag equality, dismiss if unequal\n",
    "                replacements = [elem for elem in replacements if pos_tag([elem.lower()])[0][1] == tags[idx][1]]\n",
    "                \n",
    "                #update dictionary with possible replacements for key word\n",
    "                dict.update({word:replacements}) if len(replacements) > 0 else dict\n",
    "                n = max(len(replacements), n) #update largest number of replacements\n",
    "        \n",
    "        return self.create_augmented_samples(dict, n, x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:25:07.687167Z",
     "start_time": "2020-01-05T09:25:07.678187Z"
    }
   },
   "outputs": [],
   "source": [
    "source_path = 'data/labeled_data.csv'\n",
    "target_path = 'data/augmented_data.csv'\n",
    "corpus_='glove'\n",
    "path=\"./glove/\"  # Corpus path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T09:42:50.589528Z",
     "start_time": "2020-01-05T09:25:08.016830Z"
    }
   },
   "outputs": [],
   "source": [
    "model = get_corpus(corpus_, path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-05T12:02:49.421699Z",
     "start_time": "2020-01-05T09:43:20.878287Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19600 rows successfully augmented.\n",
      "19800 rows successfully augmented.\n",
      "20000 rows successfully augmented.\n",
      "20200 rows successfully augmented.\n",
      "20400 rows successfully augmented.\n",
      "20600 rows successfully augmented.\n",
      "20800 rows successfully augmented.\n",
      "21000 rows successfully augmented.\n",
      "21200 rows successfully augmented.\n",
      "21400 rows successfully augmented.\n",
      "21600 rows successfully augmented.\n",
      "21800 rows successfully augmented.\n",
      "22000 rows successfully augmented.\n",
      "22200 rows successfully augmented.\n",
      "22400 rows successfully augmented.\n",
      "22600 rows successfully augmented.\n",
      "22800 rows successfully augmented.\n",
      "23000 rows successfully augmented.\n",
      "23200 rows successfully augmented.\n",
      "23400 rows successfully augmented.\n",
      "23600 rows successfully augmented.\n",
      "23800 rows successfully augmented.\n",
      "24000 rows successfully augmented.\n",
      "24200 rows successfully augmented.\n",
      "24400 rows successfully augmented.\n",
      "24600 rows successfully augmented.\n",
      "Augmentation complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Augment at 0x19c39dec288>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# May take several hours.\n",
    "Augment(source_path=source_path, target_path=target_path, corpus_=corpus_, path=path, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "231px",
    "width": "222px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 296,
   "position": {
    "height": "316.8px",
    "left": "1167px",
    "right": "20px",
    "top": "124px",
    "width": "326.2px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "block",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
