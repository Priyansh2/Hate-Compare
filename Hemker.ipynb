{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Replication of Hemker (2018)\n",
    "\n",
    "The goal of this notebook is to follow the methodology explained in Hemker (2018) to perform a replication of his results. Note that the source code is not available, rendering this task a bit harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Davidson et al. (2017)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/labeled_data.csv\", index_col=0)\n",
    "raw_tweets = df.tweet\n",
    "raw_labels = df[\"class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---\n",
    "\n",
    "### Noise Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"you's a muthaf***in lie &#8220;@LifeAsKing: @20_Pearls @corey_emanuel right! His TL is trash &#8230;. Now, mine? Bible scriptures and hymns&#8221;\""
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_tweets[25291]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a raw tweet:\n",
      "\"@Almightywayne__: @JetsAndASwisher @Gook____ bitch fuck u http://t.co/pXmGA68NC1\" maybe you'll get better. Just http://t.co/TPreVwfq0S\n",
      "\n",
      "Its cleaned version is:\n",
      " ||Quotation_Mark|| MENTIONHERE : MENTIONHERE MENTIONHERE bitch fuck u URLHERE ||Quotation_Mark|| maybe you'll get better ||Period|| just URLHERE \n"
     ]
    }
   ],
   "source": [
    "# Source: Davidson et al. (2017)\n",
    "\n",
    "import re\n",
    "import html\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \n",
    "    # Casing should not make a difference in our case\n",
    "    text_string = text_string.lower()\n",
    "    \n",
    "    # Regex\n",
    "    html_pattern = r'(&(?:\\#(?:(?:[0-9]+)|[Xx](?:[0-9A-Fa-f]+))|(?:[A-Za-z0-9]+));)'    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    \n",
    "    # First, add space surrounding HTML entities\n",
    "    text_string = re.sub(html_pattern, r' \\1 ', text_string)\n",
    "    \n",
    "    # Now, if we wish to find hashtags, we have to unescape HTML entities\n",
    "    text_string = html.unescape(text_string)\n",
    "    \n",
    "    # From Udacity TV script generation project\n",
    "    # Replace some punctuation by dedicated tokens\n",
    "    symbol_to_token = {\n",
    "        '.' : '||Period||',\n",
    "        ',' : '||Comma||',\n",
    "        '\"' : '||Quotation_Mark||',\n",
    "        ';' : '||Semicolon||',\n",
    "        '!' : '||Exclamation_Mark||',\n",
    "        '?' : '||Question_Mark||',\n",
    "        '(' : '||Left_Parenthesis||',\n",
    "        ')' : '||Right_Parenthesis||',\n",
    "        '-' : '||Dash||',\n",
    "        '\\n' : '||Return||'\n",
    "    }\n",
    "    \n",
    "    # Next, find URLs\n",
    "    text_string = re.sub(giant_url_regex, ' URLHERE ', text_string)\n",
    "    \n",
    "    # Then, tokenize punctuation\n",
    "    for key, token in symbol_to_token.items():\n",
    "        text_string = text_string.replace(key, ' {} '.format(token))\n",
    "\n",
    "    # Finally, remove spaces and find mentions and hashtags\n",
    "    text_string = re.sub(hashtag_regex, ' HASHTAGHERE ', text_string)\n",
    "    text_string = re.sub(mention_regex, ' MENTIONHERE ', text_string)\n",
    "    text_string = re.sub(space_pattern, ' ', text_string)\n",
    "    \n",
    "    return text_string\n",
    "\n",
    "def _test_preprocess():\n",
    "    \n",
    "    assert \" HASHTAGHERE \" == preprocess(\"#iam1hashtag\")\n",
    "    assert \" URLHERE \" == preprocess(\"https://seminar.minerva.kgi.edu\")\n",
    "    assert \" MENTIONHERE \" == preprocess(\"@vinimiranda\")\n",
    "    assert ' ' == preprocess(\"        \")\n",
    "    assert \" & MENTIONHERE URLHERE HASHTAGHERE \" == \\\n",
    "        preprocess(\"&amp;@vinimiranda    https://seminar.minerva.kgi.edu     #minerva    \")\n",
    "    \n",
    "_test_preprocess()\n",
    "\n",
    "print(\"Example of a raw tweet:\\n{}\".format(raw_tweets[68]))\n",
    "print(\"\\nIts cleaned version is:\\n{}\".format(preprocess(raw_tweets[68])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = raw_tweets.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.329, 'neu': 0.541, 'pos': 0.131, 'compound': -0.6597}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "# Example\n",
    "sentiment_analyzer.polarity_scores(tweets[68])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup table\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Udacity script project\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: Tweets\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # TODO: Implement Function\n",
    "    vocab_to_int = {word : ii for ii, word in enumerate(set(text))}    \n",
    "    int_to_vocab = {ii : word for word, ii in vocab_to_int.items()}\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "def _test_lookup_tables():\n",
    "    \n",
    "    # Make sure the dicts make the same lookup\n",
    "    missmatches = [(word, id, id, int_to_vocab[id]) for word, id in vocab_to_int.items() if int_to_vocab[id] != word]\n",
    "    \n",
    "    assert not missmatches,\\\n",
    "        'Found {} missmatche(s). First missmatch: vocab_to_int[{}] = {} and int_to_vocab[{}] = {}'.format(len(missmatches),\n",
    "                                                                                                          *missmatches[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'attacks',\n",
       " 'mango',\n",
       " 'platter',\n",
       " 'bittersweet',\n",
       " \"'lawlessness'\",\n",
       " '😒💯',\n",
       " \"95's\",\n",
       " 'muchyour',\n",
       " 'ari',\n",
       " 'drugssss',\n",
       " '48',\n",
       " 'lahhh',\n",
       " 'thotting',\n",
       " '😡😤👿🔪',\n",
       " 'lt',\n",
       " 'ninooo',\n",
       " 'jd',\n",
       " 'gloves',\n",
       " 'angelou',\n",
       " 'likin',\n",
       " 'loudmouth',\n",
       " 'dislikes',\n",
       " 'gasoline',\n",
       " 'fink',\n",
       " 'describes',\n",
       " 'nicely',\n",
       " 'hugs',\n",
       " 'provide',\n",
       " 'convoluted',\n",
       " 'cry😂😂😂',\n",
       " 'i’d',\n",
       " 'liberals',\n",
       " 'bourne',\n",
       " '😭”',\n",
       " '180k',\n",
       " 'bezel',\n",
       " 'natalie',\n",
       " 'youtube/vine/ig',\n",
       " '😭😂😂”',\n",
       " '8pm',\n",
       " 'falcons',\n",
       " 'dickheads',\n",
       " 'tornado',\n",
       " 'magician',\n",
       " '→',\n",
       " 'niggass',\n",
       " 'liver',\n",
       " 'trippen',\n",
       " 'mallett',\n",
       " 'yahweh',\n",
       " 'whoooooaaa',\n",
       " 'tears',\n",
       " 'HASHTAGHERE”😂😂',\n",
       " 'uchida',\n",
       " 'controllin',\n",
       " 'taxpayer',\n",
       " 'birkin',\n",
       " 'fuckinHASHTAGHERE',\n",
       " 'paranoia:',\n",
       " 'engels',\n",
       " 'marry',\n",
       " 'secondly',\n",
       " 'yuck',\n",
       " 'feature',\n",
       " 'benton',\n",
       " 'openwrt',\n",
       " '😂😂😂😭😭😭',\n",
       " 'trash',\n",
       " 'no”i',\n",
       " 'sub',\n",
       " 'ratchetness',\n",
       " 'daughters',\n",
       " 'frozed',\n",
       " \"barry's\",\n",
       " 'lifee',\n",
       " 'she',\n",
       " 'bait',\n",
       " 'agrees',\n",
       " 'first',\n",
       " 'alll',\n",
       " 'ny',\n",
       " 'URLHERE”flawless',\n",
       " 'hampshire',\n",
       " 'watermelon',\n",
       " 'standards',\n",
       " '83%',\n",
       " 'oop',\n",
       " 'finalist',\n",
       " 'nitty',\n",
       " 'def',\n",
       " 'pistols',\n",
       " '😒forever',\n",
       " 'spazz',\n",
       " \"luu's\",\n",
       " '30s',\n",
       " 'hoe👊😂😂',\n",
       " 'dragonball',\n",
       " 'planned',\n",
       " 'wowwww',\n",
       " \"ever'\",\n",
       " 'heaven',\n",
       " 'rebuild',\n",
       " '👠👛',\n",
       " 'include',\n",
       " 'hd',\n",
       " 'commas',\n",
       " 'virgin',\n",
       " \"john's\",\n",
       " 'november',\n",
       " 'red/mickey',\n",
       " 'cain',\n",
       " 'turns',\n",
       " 'lifeline',\n",
       " '“jinkies',\n",
       " 'loveoohyie',\n",
       " 'piled',\n",
       " 'originally',\n",
       " 'embodiment',\n",
       " 'shutting',\n",
       " 'hogan',\n",
       " 'chills',\n",
       " 'felony',\n",
       " 'pits',\n",
       " 'witta',\n",
       " 'wounds',\n",
       " 'suave',\n",
       " 'frappuccino',\n",
       " '😿”',\n",
       " 'nail',\n",
       " 'careless',\n",
       " 'endless',\n",
       " 'mvp',\n",
       " 'scent',\n",
       " 'things',\n",
       " '$30',\n",
       " 'sneaking',\n",
       " 'alexandra',\n",
       " 'struggles',\n",
       " 'twist',\n",
       " 'worries',\n",
       " 'drudgies',\n",
       " 'while…',\n",
       " 'congress',\n",
       " 'uou',\n",
       " 'no”',\n",
       " 'teamed',\n",
       " 'spank',\n",
       " 'abed',\n",
       " 'shrinks',\n",
       " 'teenage',\n",
       " 'acabó',\n",
       " 'dying',\n",
       " 'invoke',\n",
       " 'nog',\n",
       " 'highkey',\n",
       " 'speech',\n",
       " 'donjuuuan…',\n",
       " \"'what\",\n",
       " '313',\n",
       " 'ward',\n",
       " 'latex',\n",
       " 'fohead*',\n",
       " 'close',\n",
       " 'mode',\n",
       " 'wack”',\n",
       " '❤️💚',\n",
       " 'flames',\n",
       " '💣',\n",
       " \"snortin'\",\n",
       " 'tone:',\n",
       " '200',\n",
       " 'anyday',\n",
       " 'kb',\n",
       " 'mia',\n",
       " 'withou',\n",
       " 'reply😌',\n",
       " 'sneezes',\n",
       " 'masika',\n",
       " 'switched',\n",
       " '😂✌',\n",
       " 'failed',\n",
       " 'tatum',\n",
       " 'bloke',\n",
       " 'deses',\n",
       " 'ask💀💀💀rt“MENTIONHERE:',\n",
       " '💖',\n",
       " 'slim',\n",
       " \"popeye's\",\n",
       " 'stevenson',\n",
       " '😂😭😂😭',\n",
       " 'docket',\n",
       " 'inniz',\n",
       " 'over',\n",
       " 'amos',\n",
       " 'llive',\n",
       " 'jim',\n",
       " 'attitud…',\n",
       " 'ra…',\n",
       " 'stenigt',\n",
       " '😳😒',\n",
       " 'boxing',\n",
       " 'worthless',\n",
       " 'wildbills',\n",
       " '*kanye',\n",
       " 'trash*',\n",
       " 'ull',\n",
       " 'dealer',\n",
       " '😈🆗',\n",
       " 'pence',\n",
       " 'madddd',\n",
       " 'shovel',\n",
       " '🐫',\n",
       " 'guide',\n",
       " 'quick”',\n",
       " '😁😁🙌🙌🙌😃😊😜😝',\n",
       " 'leviticus',\n",
       " 'alcoholics',\n",
       " 'hookup',\n",
       " 'barking',\n",
       " \"they'd\",\n",
       " 'lutsen',\n",
       " 'sister😂💕❤️',\n",
       " 'papi',\n",
       " 'reid',\n",
       " 'pundits',\n",
       " 'thi…',\n",
       " 'yello',\n",
       " 'small',\n",
       " 'firestone',\n",
       " 'earnest',\n",
       " 'threes',\n",
       " '🔫💨🔫💨🔫💨“MENTIONHERE:',\n",
       " 'profiles',\n",
       " '\\U000feb5b\\U000fe321',\n",
       " 'whether',\n",
       " 'fajita',\n",
       " 'buncha',\n",
       " 'shut',\n",
       " 'bitch:',\n",
       " 'premature',\n",
       " 'arduino',\n",
       " 'hypocrisy',\n",
       " 'nomore',\n",
       " 'potency',\n",
       " '40',\n",
       " 'gore',\n",
       " 'biceps',\n",
       " 'uknow',\n",
       " 'carny',\n",
       " 'gutted',\n",
       " 'uninformed',\n",
       " 'kamm',\n",
       " '11th',\n",
       " 'split',\n",
       " 'mcdoubles',\n",
       " 'msg',\n",
       " 'mugged',\n",
       " 'seems',\n",
       " 'energy',\n",
       " '😬😳',\n",
       " 'freemasonry',\n",
       " 'honors:',\n",
       " 'civil',\n",
       " 'funnnnnnn',\n",
       " 'lv',\n",
       " 'redemption',\n",
       " 'brazilian',\n",
       " 'limp',\n",
       " 'taunting',\n",
       " 'luke',\n",
       " 'called',\n",
       " 'booshie',\n",
       " 'meter',\n",
       " 'ps4=a',\n",
       " 'sing',\n",
       " 'performers',\n",
       " 'extreme”',\n",
       " 'steaight',\n",
       " 'wussup',\n",
       " '👊😏👀',\n",
       " 'zoals',\n",
       " 'prowl',\n",
       " 'trina',\n",
       " 'slide',\n",
       " 'abraham',\n",
       " '😁😁😁',\n",
       " 'cunt',\n",
       " 'umpire',\n",
       " 'salad',\n",
       " 'goat',\n",
       " 'headache',\n",
       " '👽😂',\n",
       " 'hoooow',\n",
       " 'mutt',\n",
       " 'bama',\n",
       " 'bold',\n",
       " 'heals',\n",
       " 'n%a',\n",
       " '300$',\n",
       " 'weaves',\n",
       " 'infront',\n",
       " \"osama's\",\n",
       " \"'thriller'\",\n",
       " 'keef',\n",
       " 'panamonos',\n",
       " 'heard”',\n",
       " 'more”',\n",
       " 'juggling',\n",
       " 'ved',\n",
       " '🚶🚶🚶🚶🚶🚶',\n",
       " 'sponge',\n",
       " 'lifting',\n",
       " \"matt's\",\n",
       " 'amend',\n",
       " 'right',\n",
       " '👀😦😂😂😂😂😂😂😂😂😂😂💀”😂😂”💀💀💀😂',\n",
       " \"”it's\",\n",
       " 'pussayyy',\n",
       " 'afta',\n",
       " 'parks',\n",
       " 'throwed',\n",
       " 'register',\n",
       " 'frivolous',\n",
       " 'goals”…',\n",
       " 'earlier',\n",
       " 'thatch',\n",
       " 'protest',\n",
       " 'offered',\n",
       " 'martinis',\n",
       " 'moochie',\n",
       " 'cue',\n",
       " 'ge…',\n",
       " 'cryin',\n",
       " 'thot”',\n",
       " 'impeccable',\n",
       " 'btf',\n",
       " 'circled',\n",
       " 'alone',\n",
       " '*has',\n",
       " 'confirmation',\n",
       " 'cansada',\n",
       " 'season:',\n",
       " \"'community\",\n",
       " 'coon”',\n",
       " '*walks',\n",
       " 'suffers',\n",
       " '🐸☕️HASHTAGHERE…',\n",
       " '12/19',\n",
       " 'URLHERE”💀😂',\n",
       " 'la…',\n",
       " 'rounds',\n",
       " 'constitution:',\n",
       " 'clyburn',\n",
       " 'slipping',\n",
       " 'infected',\n",
       " 'tweet”',\n",
       " 'tech',\n",
       " 'mins',\n",
       " 'redonkulus',\n",
       " 'needing',\n",
       " 'mock…',\n",
       " 'nooooo',\n",
       " 'xddd',\n",
       " 'cents',\n",
       " 'niggas/bitches',\n",
       " 'ee',\n",
       " 'holding',\n",
       " 'ambassador',\n",
       " 'allowed',\n",
       " 'sav',\n",
       " 'rube',\n",
       " 'skool',\n",
       " 'pyhscopathic',\n",
       " 'shmurda',\n",
       " 'sponsor',\n",
       " 'trips',\n",
       " 'tryout',\n",
       " 'rusty',\n",
       " 'dubya',\n",
       " 'godddddd',\n",
       " 'argentino',\n",
       " 'gangsters',\n",
       " 'guides',\n",
       " 'yardy',\n",
       " 'led',\n",
       " 'deadass',\n",
       " 'beslissing',\n",
       " \"'arab\",\n",
       " 'winnin',\n",
       " 'passenger',\n",
       " '😡👊🔫',\n",
       " '😂😂😂rt',\n",
       " 'qbs',\n",
       " \"dressin'\",\n",
       " 'iph…',\n",
       " 'encourage',\n",
       " '56',\n",
       " 'slurp',\n",
       " 'store”',\n",
       " 'hardy',\n",
       " 'tooo',\n",
       " 'erickson',\n",
       " 'tho😩',\n",
       " 'campus',\n",
       " '🐛🐤',\n",
       " 'brees',\n",
       " 'brisket',\n",
       " 'helps',\n",
       " 'ranchers:',\n",
       " 'suspended',\n",
       " 'rikanya',\n",
       " 'ink',\n",
       " '”he',\n",
       " 'forgiven',\n",
       " 'pocahontas',\n",
       " '*slow',\n",
       " 'bitch☝️',\n",
       " 'shoving',\n",
       " 'maggie',\n",
       " 'eattin',\n",
       " 'socket',\n",
       " 'bitchy',\n",
       " 'upon',\n",
       " 'grandpa',\n",
       " 'redskin',\n",
       " 'undermines',\n",
       " 'socked',\n",
       " 'vanish',\n",
       " 'seen😤',\n",
       " 'yawl',\n",
       " '💏😂',\n",
       " '😴😴😴',\n",
       " 'ahora',\n",
       " 'past',\n",
       " 'reason',\n",
       " 'happin',\n",
       " '😥”hoes',\n",
       " 'von',\n",
       " ':',\n",
       " 'sketchball',\n",
       " \"katy's\",\n",
       " 'waived',\n",
       " 'ol',\n",
       " 'traitor',\n",
       " \"d'usse\",\n",
       " 'chappelle',\n",
       " 'clearer',\n",
       " 'olive',\n",
       " 'homeboys',\n",
       " 'film',\n",
       " 'phone*',\n",
       " 'joon',\n",
       " 'erything',\n",
       " 'irl',\n",
       " '”ms',\n",
       " 'retards…',\n",
       " '✋😂😏',\n",
       " 'felecia',\n",
       " 'ioc',\n",
       " 'speedwagon',\n",
       " 'voz',\n",
       " 'humpday',\n",
       " '❤️',\n",
       " 'nuggets',\n",
       " 'bitch😈',\n",
       " 'virgo',\n",
       " \"motherfuckin'\",\n",
       " 'trays',\n",
       " 'soo',\n",
       " 'subhuman',\n",
       " 'receipts',\n",
       " '12th',\n",
       " 'maas',\n",
       " 'sum',\n",
       " 'away”',\n",
       " 'sheets',\n",
       " '*writes',\n",
       " 'vulgar',\n",
       " 'balkamania',\n",
       " '💃💃💃💃💃💃💃',\n",
       " 'steez',\n",
       " 'stiff',\n",
       " 'ebolian',\n",
       " 'downed',\n",
       " 'skinned',\n",
       " 'bothers',\n",
       " 'coughing',\n",
       " '👏👏👏👏🙌',\n",
       " 'branding',\n",
       " 'replies',\n",
       " 'cheesed',\n",
       " 'lying😕',\n",
       " 'thugnasty',\n",
       " 'puddy',\n",
       " 'carrot',\n",
       " 'shotgunned',\n",
       " 'thugga',\n",
       " 'explosions',\n",
       " 'gouged',\n",
       " 'toes…',\n",
       " 'too😂✌️',\n",
       " 'collossal',\n",
       " 'micheals',\n",
       " '0_0',\n",
       " 'uhh',\n",
       " 'mongerls',\n",
       " 'listeners',\n",
       " 'pant…',\n",
       " \"amaris'\",\n",
       " 'gmfu',\n",
       " 'bitch”',\n",
       " 'compete',\n",
       " 'ganarle',\n",
       " 'summfest”',\n",
       " '*muslim',\n",
       " 'simpsons',\n",
       " 'integrity',\n",
       " 'ewwww',\n",
       " 'lol',\n",
       " 'vestal',\n",
       " 'thissssssss',\n",
       " 'phil',\n",
       " 'maar',\n",
       " 'betterthanamobster',\n",
       " '15%',\n",
       " '2parasites',\n",
       " 'fads',\n",
       " 'impressions',\n",
       " 'trueeee',\n",
       " 'that',\n",
       " 'likkadawgg',\n",
       " 'afl',\n",
       " 'actors',\n",
       " 'older',\n",
       " 'fuckn',\n",
       " 'shorts',\n",
       " 'returns',\n",
       " 'chanting',\n",
       " 'lou',\n",
       " 'nicks',\n",
       " 'chrisis',\n",
       " 'banged',\n",
       " 'nano<<<<<',\n",
       " 'jesus',\n",
       " 'looooool😭',\n",
       " 'bitch”MENTIONHERE',\n",
       " 'pounds',\n",
       " 'cook',\n",
       " 'ordination',\n",
       " 'convenient',\n",
       " 'bansal/gadkari',\n",
       " 'relo',\n",
       " 'assault',\n",
       " \"'dodo'\",\n",
       " 'sill',\n",
       " 'sextape',\n",
       " 'discipline',\n",
       " 'heisenberg',\n",
       " 'burns',\n",
       " 'tracy',\n",
       " \"goose's\",\n",
       " 'considering',\n",
       " 'interior',\n",
       " 'm4w',\n",
       " '60k',\n",
       " 'monkey/bear',\n",
       " 'falicia',\n",
       " 'anbar',\n",
       " 'fags',\n",
       " 'mm',\n",
       " 'initiate',\n",
       " 'academic',\n",
       " 'unswole',\n",
       " 'chestnut',\n",
       " 'esophagus',\n",
       " 'throat',\n",
       " 'eyed',\n",
       " 'qualitaties',\n",
       " 'suppose',\n",
       " 'traveling',\n",
       " 'stopp',\n",
       " 'dumber',\n",
       " 'zog',\n",
       " '✌️😒',\n",
       " 'stands',\n",
       " 'gamma',\n",
       " 'idiocy',\n",
       " 'lauren',\n",
       " 'alg…',\n",
       " '⌚💂',\n",
       " 'rocawe…',\n",
       " 'motha',\n",
       " 'belligerent',\n",
       " 'stephen',\n",
       " '◡̈⃝',\n",
       " \"turner's\",\n",
       " 'deez',\n",
       " 'eddie',\n",
       " 'likeeee',\n",
       " 'swiggety',\n",
       " 'upload',\n",
       " 'hypocrites',\n",
       " 'nemo',\n",
       " 'feeling',\n",
       " 'tom/sell',\n",
       " 'faaaaggggottttt',\n",
       " 'huskies',\n",
       " 'uninvited',\n",
       " 'given:',\n",
       " 'honkey',\n",
       " 'moves',\n",
       " 'speaking',\n",
       " 'scarce',\n",
       " ':p',\n",
       " 'easily',\n",
       " 'soundtrack',\n",
       " 'chinatown',\n",
       " '😂😂😴😴😴😴💤✋✌️',\n",
       " 'ltao',\n",
       " 'spewing',\n",
       " 'elsewhere',\n",
       " \"kayne's\",\n",
       " 'civic',\n",
       " 'sported',\n",
       " \"'distribution\",\n",
       " '*rubs',\n",
       " 'biggie',\n",
       " 'happening',\n",
       " 'puntjebijpaaltje',\n",
       " 'worse',\n",
       " 'sandy',\n",
       " 'could',\n",
       " 'goproud',\n",
       " 'bleach',\n",
       " 'team😐',\n",
       " 'cheez',\n",
       " 'cali',\n",
       " 'soles',\n",
       " 'decoy',\n",
       " '🙊',\n",
       " 'wheat',\n",
       " 'bowyer',\n",
       " 'birdcage',\n",
       " 'napkin',\n",
       " 'luhnow',\n",
       " '21st',\n",
       " '😂✋',\n",
       " 'allen',\n",
       " 'navajo',\n",
       " 'propose',\n",
       " '“6',\n",
       " 'fellas',\n",
       " '\\U000fe343✌️',\n",
       " 'macin',\n",
       " 'rulers',\n",
       " 'clout',\n",
       " 'jammin',\n",
       " 'district',\n",
       " 'appearances',\n",
       " 'hoe”thot',\n",
       " \"'smackin\",\n",
       " 'peterson',\n",
       " 'minutes',\n",
       " 'perfectionist',\n",
       " 'w/',\n",
       " 'shoutouts',\n",
       " 'skim',\n",
       " 'heh',\n",
       " '100%',\n",
       " 'elissa',\n",
       " 'abs',\n",
       " 'disguise',\n",
       " '😳😩😂😂',\n",
       " '😏',\n",
       " 'e',\n",
       " \"jimmy's\",\n",
       " '>>>>>',\n",
       " 'wackkkkk',\n",
       " 'bitching',\n",
       " 'cory',\n",
       " 'layer',\n",
       " 'smite',\n",
       " 'warm',\n",
       " 'motown',\n",
       " 'kbye',\n",
       " 'salivating',\n",
       " 'slaaaayy',\n",
       " '😂😂😂😩✌️',\n",
       " 'scandalous',\n",
       " 'sandpiper',\n",
       " 'versi',\n",
       " 'yard',\n",
       " 'bumcia',\n",
       " 'sc',\n",
       " 'percocet',\n",
       " 'sumblime',\n",
       " 'marca',\n",
       " 'goodbye',\n",
       " 'california',\n",
       " 'hsn',\n",
       " 'wildcat',\n",
       " 'tell',\n",
       " 'don’t',\n",
       " 'toein',\n",
       " 'pricless',\n",
       " 'dawn',\n",
       " 'dispersed',\n",
       " 'cob',\n",
       " '&🐫*',\n",
       " 'minstrel',\n",
       " 'downtown',\n",
       " 'moans',\n",
       " 'chick/',\n",
       " 'wack',\n",
       " 'pornos',\n",
       " 'uppercut',\n",
       " 'ihy',\n",
       " 'ras',\n",
       " 'adidas',\n",
       " 'sir',\n",
       " 'hoes😒👌',\n",
       " 'zolang',\n",
       " '💸💯',\n",
       " 'win',\n",
       " 'peen',\n",
       " 'konnnichiwa',\n",
       " 'shrek',\n",
       " 'wana',\n",
       " 'verbal',\n",
       " \"twinkie's\",\n",
       " 'incident',\n",
       " 'gf👫',\n",
       " 'writes',\n",
       " 'depot',\n",
       " 'passes',\n",
       " 'bi…',\n",
       " 'bowl',\n",
       " 'upenn',\n",
       " 'sitting',\n",
       " 'sec',\n",
       " 'cencus',\n",
       " 'kemora',\n",
       " 'wayans',\n",
       " '5am',\n",
       " 'sex😎💯',\n",
       " 'tonight”',\n",
       " 'liberty',\n",
       " 'asides”lmao',\n",
       " 'democrat',\n",
       " 'daniel',\n",
       " 'pessimistic',\n",
       " 'bumpin',\n",
       " 'gie',\n",
       " '😷🙊👅🙅',\n",
       " 'nikki',\n",
       " 'raw',\n",
       " 'danger',\n",
       " 'waaaaannnnnt',\n",
       " '/',\n",
       " 'fite',\n",
       " 'torres',\n",
       " 'obamas',\n",
       " 'troops',\n",
       " 'subtweettweettweettweet',\n",
       " 'repcon',\n",
       " 'needed',\n",
       " 'ordinary',\n",
       " 'caveman',\n",
       " 'prioritized',\n",
       " 'henry',\n",
       " 'benghazi',\n",
       " 'bibles',\n",
       " 'scho',\n",
       " 'unkept',\n",
       " 'track',\n",
       " 'rattlers',\n",
       " 'pounder',\n",
       " 'protective',\n",
       " \"victoria's\",\n",
       " 'perkins',\n",
       " 'wetback',\n",
       " 'college',\n",
       " 'slag',\n",
       " '🎨',\n",
       " 'russians',\n",
       " 'vonnie',\n",
       " 'minds',\n",
       " '😳🙈”',\n",
       " 'drivers',\n",
       " 'audience',\n",
       " \"sheffield's\",\n",
       " '2/balt',\n",
       " 'backing',\n",
       " 'bohner',\n",
       " 'pow',\n",
       " 'rooting',\n",
       " 'speakers',\n",
       " 'weef',\n",
       " 'sammich',\n",
       " 'beginning',\n",
       " 'stocking',\n",
       " 'URLHERE”😩😩😂',\n",
       " 'astronaut',\n",
       " 'yasssss',\n",
       " 'range',\n",
       " '🙋',\n",
       " 'corpse',\n",
       " 'rewrite',\n",
       " 'full',\n",
       " '😷',\n",
       " 'elves',\n",
       " 'agony',\n",
       " 'rob',\n",
       " 'valerie',\n",
       " 'cheeseburger',\n",
       " 'ofay',\n",
       " 'announced',\n",
       " 'idiots',\n",
       " 'blaming',\n",
       " 'welp',\n",
       " '💯💯💯💯💯💯💯',\n",
       " 'sweating',\n",
       " '“insults”',\n",
       " 'sex”',\n",
       " 'r/t',\n",
       " 'URLHERE”😳😧',\n",
       " 'eeeeeeeee',\n",
       " 'bons',\n",
       " 'prolly',\n",
       " 'uppers',\n",
       " \"@'d\",\n",
       " 'flattered',\n",
       " 'gym',\n",
       " 'backpacks',\n",
       " 'declares',\n",
       " 'ohioans',\n",
       " 'sheet',\n",
       " 'stafford',\n",
       " 'euro',\n",
       " \"'dying\",\n",
       " 'honking',\n",
       " 'jumpers',\n",
       " 'morning',\n",
       " 'hostel',\n",
       " 'asaka´s',\n",
       " 'ouutttt',\n",
       " 'reject',\n",
       " 'once…',\n",
       " 'fuckboys',\n",
       " 'egregious',\n",
       " 'hospitality',\n",
       " 'muzzled',\n",
       " 'hesitation',\n",
       " '😂😂😂😂😭😭😭😭rt',\n",
       " 'balla',\n",
       " 'gallons',\n",
       " 'pharr',\n",
       " 'missy',\n",
       " 'car',\n",
       " \"mccain's\",\n",
       " \"beverley's\",\n",
       " 'home',\n",
       " 'resign',\n",
       " 'hobbits',\n",
       " 'polite',\n",
       " \"30's»\",\n",
       " 'commend',\n",
       " 'technically',\n",
       " 'gang:',\n",
       " '🏈',\n",
       " 'nick_swag_is_504',\n",
       " 'daft',\n",
       " 'chilli',\n",
       " '🔫🔫🔫',\n",
       " 'frank',\n",
       " 'strapped',\n",
       " 'URLHERE”lets',\n",
       " 'holiday',\n",
       " 'tends',\n",
       " 'deshawn',\n",
       " 'scripted',\n",
       " 'trent',\n",
       " 'back”',\n",
       " 'hebronville',\n",
       " 'dancehall',\n",
       " 'interview',\n",
       " 'bites',\n",
       " 'groom',\n",
       " 'lawyering',\n",
       " '2015',\n",
       " 'joan',\n",
       " 'milkshake',\n",
       " 'game7',\n",
       " 'madonna',\n",
       " 'rabies',\n",
       " 'ferderline',\n",
       " 'taveras',\n",
       " '…',\n",
       " '1776',\n",
       " 'treasures',\n",
       " 'somehow',\n",
       " 'hates',\n",
       " '😩😭😂”',\n",
       " 'kobe',\n",
       " 'laggin',\n",
       " 'defiantly',\n",
       " 'memorabilia',\n",
       " \"lamar's\",\n",
       " 'intelligent',\n",
       " 'script',\n",
       " 'yoburger',\n",
       " 'hooouseee',\n",
       " 'aha',\n",
       " 'texting',\n",
       " 'gots',\n",
       " '“fuck',\n",
       " 'faves:',\n",
       " '💁everybody',\n",
       " 'haslam',\n",
       " 'haley',\n",
       " 'unpredictable',\n",
       " 'emosquad2k10',\n",
       " 'hdm',\n",
       " 'ponzi',\n",
       " 'hoop',\n",
       " 'eerst',\n",
       " 'tricks',\n",
       " 'vmas',\n",
       " 'biking',\n",
       " 'hershey',\n",
       " 'pipe',\n",
       " 'pilgrimage',\n",
       " 'insult',\n",
       " 'yal',\n",
       " 'hazard',\n",
       " 'impersonators',\n",
       " \"davis'\",\n",
       " 'valued',\n",
       " \"pickin'\",\n",
       " 'defecate',\n",
       " 'latter',\n",
       " '👌💸💵',\n",
       " 'w/his',\n",
       " 'rehte',\n",
       " 'happend',\n",
       " 'do/mob',\n",
       " 'gill',\n",
       " 'conductor',\n",
       " '“anarchism',\n",
       " '\\U000fe334\\U000fe347\\U000fe32c',\n",
       " 'sunshine',\n",
       " 'patron:',\n",
       " 'grade',\n",
       " 'him’:',\n",
       " '\\U000fe334\\U000feb7b\\U000fe32c',\n",
       " 'wicked',\n",
       " 'golds',\n",
       " 'one=pancake',\n",
       " 'separate',\n",
       " 'grandbabies',\n",
       " 'showman',\n",
       " '~>',\n",
       " 'rams',\n",
       " 'intended',\n",
       " 'saviour',\n",
       " 'pugs',\n",
       " '“pussy”',\n",
       " 'peeped',\n",
       " 'scampi',\n",
       " '💁😂😂😂',\n",
       " 'dnd',\n",
       " 'reminiscin',\n",
       " 'accuracy',\n",
       " 'animals',\n",
       " '20pics',\n",
       " 'beurs',\n",
       " 'happened',\n",
       " 'politico',\n",
       " 'winking',\n",
       " 'wait',\n",
       " 'niggas😵😂😂',\n",
       " '*wall',\n",
       " 'startin',\n",
       " '😂😂😂bitch',\n",
       " 'fuccd',\n",
       " 'suck',\n",
       " 'english',\n",
       " 'specialty',\n",
       " 'babe',\n",
       " 'checkout',\n",
       " 'sotloff',\n",
       " 'talkinfgalot',\n",
       " 'wwn',\n",
       " 'scorpion',\n",
       " '🎉🔫',\n",
       " 'castle',\n",
       " \"park's\",\n",
       " 'ape😅😂',\n",
       " 'disses>>',\n",
       " 'reminds',\n",
       " ...}"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = set()\n",
    "tweets.str.split().apply(vocab.update)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hate Subclass Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partly from https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list\n",
    "# I do not implement co-reference resolution since a single NE is sufficient for directed hate speech.\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "def hate_classification(hate_tweet):\n",
    "    '''Receives a hateful tweet. \n",
    "       Return 3 for directed hate speech and 4 otherwise.'''\n",
    "    \n",
    "    if bool(hate_tweet.count(\"MENTIONHERE\")): return(3)\n",
    "    \n",
    "    # Remove tokens since they will oncused the POS tagger\n",
    "    token_regex = '\\|\\|\\w+\\|\\|'\n",
    "    hate_tweet = re.sub(token_regex, \"\", hate_tweet)\n",
    "    \n",
    "    # URLHERE is considered a proper noun by the pos tagger.\n",
    "    # Remove them before checking for proper nouns\n",
    "    no_punct_hate = ''.join([char for char in hate_tweet if char not in punctuation])\n",
    "    no_URL_hate = ' '.join([token for token in no_punct_hate.split() if token != \"URLHERE\"])\n",
    "    has_NE = False\n",
    "    for sent in nltk.sent_tokenize(no_URL_hate):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                return(3)  # Named Entity found    \n",
    "\n",
    "    return(4)\n",
    "        \n",
    "def _test_hate_classification():\n",
    "    assert hate_classification(\"MENTIONHERE\") == 3\n",
    "    assert hate_classification(\"Karen is absolutely crazy\") == 3\n",
    "    assert hate_classification(\"Karen is his sister. She's absolutely crazy\") == 3\n",
    "    assert hate_classification(\"They should all be sent to Mexico\") == 3\n",
    "    assert hate_classification(\"They should all leave the country\") == 4\n",
    "    assert hate_classification(\"some hate speech stuff\") == 4\n",
    "    assert hate_classification(\"\") == 4\n",
    "\n",
    "_test_hate_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a hateful tweet: \n",
      " ||Quotation_Mark|| we're out here ||Comma|| and we're queer ||Exclamation_Mark|| ||Quotation_Mark|| ||Return|| ||Quotation_Mark|| 2 ||Comma|| 4 ||Comma|| 6 ||Comma|| hut ||Exclamation_Mark|| we like it in our butt ||Exclamation_Mark|| ||Quotation_Mark|| \n",
      "Its type of hate speech is: Generalized\n",
      "\n",
      "Example of a hateful tweet:\n",
      " ||Quotation_Mark|| MENTIONHERE: jackies a retard HASHTAGHERE ||Quotation_Mark|| at least i can make a grilled cheese ||Exclamation_Mark|| \n",
      "Its type of hate speech is: Directed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hate_tweets = tweets[df[\"class\"] == 0].values\n",
    "_hate_prnt = lambda x : \"Generalized\" if hate_classification(x) == 4 else \"Directed\"\n",
    "\n",
    "print(\"Example of a hateful tweet: \\n{}\".format(hate_tweets[20]))\n",
    "print(\"Its type of hate speech is: {}\\n\".format(_hate_prnt(hate_tweets[20])))\n",
    "\n",
    "print(\"Example of a hateful tweet:\\n{}\".format(hate_tweets[10]))\n",
    "print(\"Its type of hate speech is: {}\\n\".format(_hate_prnt(hate_tweets[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change hate speech labels (0) to directed (3) / generalized labels (4) \n",
    "labels = raw_labels.copy()\n",
    "for i, (tweet, label) in enumerate(zip(tweets, raw_labels)):\n",
    "    \n",
    "    if label == 0:  # If hate speech\n",
    "        labels[i] = hate_classification(tweet)\n",
    "\n",
    "def _test_labels():\n",
    "    assert 1 not in pd.Series(labels).value_counts().index\n",
    "    assert 3 in pd.Series(labels).value_counts().index\n",
    "    assert 4 in pd.Series(labels).value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19190\n",
       "2     4163\n",
       "3     1183\n",
       "4      247\n",
       "dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "---\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
