{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Replication of Hemker (2018)\n",
    "\n",
    "The goal of this notebook is to follow the methodology explained in Hemker (2018) to perform a replication of his results. Note that the source code is not available, rendering this task a bit harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Davidson et al. (2017)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/labeled_data.csv\", index_col=0)\n",
    "raw_tweets = df.tweet\n",
    "raw_labels = df[\"class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---\n",
    "\n",
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a raw tweet:\n",
      "\"@Almightywayne__: @JetsAndASwisher @Gook____ bitch fuck u http://t.co/pXmGA68NC1\" maybe you'll get better. Just http://t.co/TPreVwfq0S\n",
      "\n",
      "Its cleaned version is:\n",
      " ||Quotation_Mark|| MENTIONHERE : MENTIONHERE MENTIONHERE bitch fuck u URLHERE ||Quotation_Mark|| maybe you'll get better ||Period|| just URLHERE \n"
     ]
    }
   ],
   "source": [
    "# Source: Davidson et al. (2017)\n",
    "\n",
    "import re\n",
    "import html\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \n",
    "    # Casing should not make a difference in our case\n",
    "    text_string = text_string.lower()\n",
    "    \n",
    "    # Regex\n",
    "    html_pattern = r'(&(?:\\#(?:(?:[0-9]+)|[Xx](?:[0-9A-Fa-f]+))|(?:[A-Za-z0-9]+));)'    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    \n",
    "    # First, add space surrounding HTML entities\n",
    "    text_string = re.sub(html_pattern, r' \\1 ', text_string)\n",
    "    \n",
    "    # Now, if we wish to find hashtags, we have to unescape HTML entities\n",
    "    text_string = html.unescape(text_string)\n",
    "    \n",
    "    # From Udacity TV script generation project\n",
    "    # Replace some punctuation by dedicated tokens\n",
    "    symbol_to_token = {\n",
    "        '.' : '||Period||',\n",
    "        ',' : '||Comma||',\n",
    "        '\"' : '||Quotation_Mark||',\n",
    "        ';' : '||Semicolon||',\n",
    "        '!' : '||Exclamation_Mark||',\n",
    "        '?' : '||Question_Mark||',\n",
    "        '(' : '||Left_Parenthesis||',\n",
    "        ')' : '||Right_Parenthesis||',\n",
    "        '-' : '||Dash||',\n",
    "        '\\n' : '||Return||'\n",
    "    }\n",
    "    \n",
    "    # Next, find URLs\n",
    "    text_string = re.sub(giant_url_regex, ' URLHERE ', text_string)\n",
    "    \n",
    "    # Then, tokenize punctuation\n",
    "    for key, token in symbol_to_token.items():\n",
    "        text_string = text_string.replace(key, ' {} '.format(token))\n",
    "\n",
    "    # Finally, remove spaces and find mentions and hashtags\n",
    "    text_string = re.sub(hashtag_regex, ' HASHTAGHERE ', text_string)\n",
    "    text_string = re.sub(mention_regex, ' MENTIONHERE ', text_string)\n",
    "    text_string = re.sub(space_pattern, ' ', text_string)\n",
    "    \n",
    "    return text_string\n",
    "\n",
    "def _test_preprocess():\n",
    "    \n",
    "    assert \" HASHTAGHERE \" == preprocess(\"#iam1hashtag\")\n",
    "    assert \" URLHERE \" == preprocess(\"https://seminar.minerva.kgi.edu\")\n",
    "    assert \" MENTIONHERE \" == preprocess(\"@vinimiranda\")\n",
    "    assert ' ' == preprocess(\"        \")\n",
    "    assert \" & MENTIONHERE URLHERE HASHTAGHERE \" == \\\n",
    "        preprocess(\"&amp;@vinimiranda    https://seminar.minerva.kgi.edu     #minerva    \")\n",
    "    \n",
    "_test_preprocess()\n",
    "\n",
    "print(\"Example of a raw tweet:\\n{}\".format(raw_tweets[68]))\n",
    "print(\"\\nIts cleaned version is:\\n{}\".format(preprocess(raw_tweets[68])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = raw_tweets.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.329, 'neu': 0.541, 'pos': 0.131, 'compound': -0.6597}"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "# Example\n",
    "sentiment_analyzer.polarity_scores(tweets[68])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    24783.000000\n",
       "mean        16.729936\n",
       "std          8.445555\n",
       "min          1.000000\n",
       "25%         10.000000\n",
       "50%         16.000000\n",
       "75%         23.000000\n",
       "max         95.000000\n",
       "Name: word_count, dtype: float64"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get cleaned tweets\n",
    "df[\"clean_tweet\"] = tweets\n",
    "\n",
    "# Get their word count\n",
    "df[\"word_count\"] = df.clean_tweet.apply(lambda x : len(x.split()))\n",
    "\n",
    "df.word_count.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>821</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>#Yankees</td>\n",
       "      <td>HASHTAGHERE</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24147</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>bitches</td>\n",
       "      <td>bitches</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24218</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>coons</td>\n",
       "      <td>coons</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24869</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>pussy</td>\n",
       "      <td>pussy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class     tweet  \\\n",
       "821        3            0                   0        3      2  #Yankees   \n",
       "24147      3            0                   3        0      1   bitches   \n",
       "24218      3            3                   0        0      0     coons   \n",
       "24869      3            0                   3        0      1     pussy   \n",
       "\n",
       "         clean_tweet  word_count  \n",
       "821     HASHTAGHERE            1  \n",
       "24147        bitches           1  \n",
       "24218          coons           1  \n",
       "24869          pussy           1  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tweets with the minimum word count\n",
    "df.loc[df.word_count == df.word_count.min(),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good. Let's check the tweet(s) with the maximum word count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>22953</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Was finna slit my eyebrows up in the shop but ...</td>\n",
       "      <td>was finna slit my eyebrows up in the shop but ...</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "22953      3            0                   0        3      2   \n",
       "\n",
       "                                                   tweet  \\\n",
       "22953  Was finna slit my eyebrows up in the shop but ...   \n",
       "\n",
       "                                             clean_tweet  word_count  \n",
       "22953  was finna slit my eyebrows up in the shop but ...          95  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tweets with the maximum length\n",
    "df.loc[df.word_count == df.word_count.max(),]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's something strange going on. Let's check the tweet again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Was finna slit my eyebrows up in the shop but nahhhhhh.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.\\r\\n.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.word_count == df.word_count.max(),].tweet.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tweet contains a lot of new lines. It's hard to know why, but I'll choose to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_tweet = df.loc[df.word_count == df.word_count.max(),].tweet.values[0]\n",
    "new_tweet = old_tweet[:old_tweet.find(\"\\r\")]\n",
    "df.loc[df.word_count == df.word_count.max(), \"tweet\"] = new_tweet\n",
    "df.loc[df.word_count == df.word_count.max(), \"clean_tweet\"] = preprocess(new_tweet)\n",
    "df.loc[df.word_count == df.word_count.max(), \"word_count\"] = len(preprocess(new_tweet).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>18267</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @TrxllLegend: One good girl is worth a thou...</td>\n",
       "      <td>rt MENTIONHERE : one good girl is worth a thou...</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       count  hate_speech  offensive_language  neither  class  \\\n",
       "18267      3            0                   3        0      1   \n",
       "\n",
       "                                                   tweet  \\\n",
       "18267  RT @TrxllLegend: One good girl is worth a thou...   \n",
       "\n",
       "                                             clean_tweet  word_count  \n",
       "18267  rt MENTIONHERE : one good girl is worth a thou...          91  "
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check tweets with the maximum length\n",
    "df.loc[df.word_count == df.word_count.max(),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt MENTIONHERE : one good girl is worth a thousand bitches ||Return|| ||Return|| 👰 = 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 👭 … '"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.word_count == df.word_count.max(),].clean_tweet.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sighes. Well, format-wise it is okay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Udacity script project\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: Tweets\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate vocabulary\n",
    "    vocab = set()\n",
    "    tweets.str.split().apply(vocab.update)\n",
    "    vocab.update(set(SPECIAL_WORDS.values()))\n",
    "    \n",
    "    # Generate lookup tables\n",
    "    vocab_to_int = {word : ii for ii, word in enumerate(vocab, 1)}    \n",
    "    int_to_vocab = {ii : word for word, ii in vocab_to_int.items()}\n",
    "    \n",
    "    # Add padding special word\n",
    "    vocab_to_int['<PAD>'] = 0\n",
    "    int_to_vocab[0] = '<PAD>'\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "def _test_lookup_tables():\n",
    "    \n",
    "    text = np.array([\"this is a toy\", \"I mean not really a toy\", \"I mean a toy vocabulary\"])\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "    \n",
    "    # Make sure the dicts make the same lookup\n",
    "    missmatches = [(word, id, id, int_to_vocab[id]) for word, id in vocab_to_int.items() if int_to_vocab[id] != word]\n",
    "    \n",
    "    assert not missmatches,\\\n",
    "        'Found {} missmatche(s). First missmatch: vocab_to_int[{}] = {} and int_to_vocab[{}] = {}'.format(len(missmatches),\n",
    "                                                                                                          *missmatches[0])\n",
    "    \n",
    "_test_lookup_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is: 21134 tokens.\n",
      "These are 10 randomly sample words in the vocabulary:\n",
      "['afl', 'seat', 'creatures', 'purnt', 'faggots', 'lyrical', 'aliens', 'suit', 'ne', 'larger']\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the vocabulary is: {} tokens.\".format(len(vocab_to_int)))\n",
    "vocab = list(vocab_to_int.keys())\n",
    "np.random.shuffle(vocab)\n",
    "print(\"These are 10 randomly sample words in the vocabulary:\\n{}\".format(vocab[:10]))\n",
    "del vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Padding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = df.word_count.max()\n",
    "\n",
    "def pad_tweets(tweet, max_length=MAX_LENGTH):\n",
    "    # Do not cut tweet short if it's too long\n",
    "\n",
    "    # Retrieve tweet word count\n",
    "    word_count = len(tweet.split())\n",
    "    \n",
    "    # Check how much padding will be needed\n",
    "    n = max_length - word_count if word_count < max_length else 0\n",
    "\n",
    "    # Pad tweet\n",
    "    padded_tweet = ''.join(['<PAD> '] * n + [tweet])\n",
    "   \n",
    "    return padded_tweet\n",
    "\n",
    "def _test_pad_tweets():\n",
    "    \n",
    "    assert pad_tweets('hi', 0) == 'hi'\n",
    "    assert pad_tweets('hi', 1) == 'hi'\n",
    "    assert pad_tweets('hi', 2) == '<PAD> hi'\n",
    "    assert len(pad_tweets('hi', 10).split()) == 10\n",
    "    assert len(pad_tweets('hi', 100).split()) == 100\n",
    "    assert pad_tweets('this sentence is a bit longer', 1) == 'this sentence is a bit longer'\n",
    "    \n",
    "_test_pad_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>  ||Quotation_Mark|| keeks is a bitch she curves everyone ||Quotation_Mark|| lol i walked into a conversation like this ||Period|| smh\n"
     ]
    }
   ],
   "source": [
    "df[\"padded_tweets\"] = df.clean_tweet.map(pad_tweets)\n",
    "print(df.padded_tweets[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "  5903 18881  7952  5450 15483 11510 20674 12309  5903  1633 19264 14962\n",
      "  9032  5450 10578 19895 14569  9270 21019]\n"
     ]
    }
   ],
   "source": [
    "tweets_ints = np.array([[vocab_to_int[word] for word in tweet.split()] for tweet in df.padded_tweets.values])\n",
    "print(tweets_ints[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hate Subclass Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partly from https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list\n",
    "# I do not implement co-reference resolution since a single NE is sufficient for directed hate speech.\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "def hate_classification(hate_tweet):\n",
    "    '''Receives a hateful tweet. \n",
    "       Return 3 for directed hate speech and 4 otherwise.'''\n",
    "    \n",
    "    if bool(hate_tweet.count(\"MENTIONHERE\")): return(3)\n",
    "    \n",
    "    # Remove tokens since they will oncused the POS tagger\n",
    "    token_regex = '\\|\\|\\w+\\|\\|'\n",
    "    hate_tweet = re.sub(token_regex, \"\", hate_tweet)\n",
    "    \n",
    "    # URLHERE is considered a proper noun by the pos tagger.\n",
    "    # Remove them before checking for proper nouns\n",
    "    no_punct_hate = ''.join([char for char in hate_tweet if char not in punctuation])\n",
    "    no_URL_hate = ' '.join([token for token in no_punct_hate.split() if token != \"URLHERE\"])\n",
    "    has_NE = False\n",
    "    for sent in sent_tokenize(no_URL_hate):\n",
    "        for chunk in ne_chunk(pos_tag(word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                return(3)  # Named Entity found    \n",
    "\n",
    "    return(4)\n",
    "        \n",
    "def _test_hate_classification():\n",
    "    assert hate_classification(\"MENTIONHERE\") == 3\n",
    "    assert hate_classification(\"Karen is absolutely crazy\") == 3\n",
    "    assert hate_classification(\"Karen is his sister. She's absolutely crazy\") == 3\n",
    "    assert hate_classification(\"They should all be sent to Mexico\") == 3\n",
    "    assert hate_classification(\"They should all leave the country\") == 4\n",
    "    assert hate_classification(\"some hate speech stuff\") == 4\n",
    "    assert hate_classification(\"\") == 4\n",
    "\n",
    "_test_hate_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a hateful tweet: \n",
      " ||Quotation_Mark|| we're out here ||Comma|| and we're queer ||Exclamation_Mark|| ||Quotation_Mark|| ||Return|| ||Quotation_Mark|| 2 ||Comma|| 4 ||Comma|| 6 ||Comma|| hut ||Exclamation_Mark|| we like it in our butt ||Exclamation_Mark|| ||Quotation_Mark|| \n",
      "Its type of hate speech is: Generalized\n",
      "\n",
      "Example of a hateful tweet:\n",
      " ||Quotation_Mark|| MENTIONHERE : jackies a retard HASHTAGHERE ||Quotation_Mark|| at least i can make a grilled cheese ||Exclamation_Mark|| \n",
      "Its type of hate speech is: Directed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hate_tweets = tweets[df[\"class\"] == 0].values\n",
    "_hate_prnt = lambda x : \"Generalized\" if hate_classification(x) == 4 else \"Directed\"\n",
    "\n",
    "print(\"Example of a hateful tweet: \\n{}\".format(hate_tweets[20]))\n",
    "print(\"Its type of hate speech is: {}\\n\".format(_hate_prnt(hate_tweets[20])))\n",
    "\n",
    "print(\"Example of a hateful tweet:\\n{}\".format(hate_tweets[10]))\n",
    "print(\"Its type of hate speech is: {}\\n\".format(_hate_prnt(hate_tweets[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change hate speech labels (0) to directed (3) / generalized labels (4) \n",
    "labels = raw_labels.copy()\n",
    "for i, (tweet, label) in enumerate(zip(tweets, raw_labels)):\n",
    "    \n",
    "    if label == 0:  # If hate speech\n",
    "        labels[i] = hate_classification(tweet)\n",
    "\n",
    "def _test_labels():\n",
    "    assert 1 not in pd.Series(labels).value_counts().index\n",
    "    assert 3 in pd.Series(labels).value_counts().index\n",
    "    assert 4 in pd.Series(labels).value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19190\n",
       "2     4163\n",
       "3      954\n",
       "4      476\n",
       "dtype: int64"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "---\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Training, Validation, and Test Sets\n",
    "\n",
    "all from Udacity script generator project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(19826, 91) \n",
      "Validation set: \t(2478, 91) \n",
      "Test set: \t\t(2479, 91)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "tweets_ints, labels = shuffle(tweets_ints, labels)\n",
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "split_idx = int(tweets_ints.shape[0]*split_frac)\n",
    "train_x, remaining_x = tweets_ints[:split_idx], tweets_ints[split_idx:]\n",
    "train_y, remaining_y = labels[:split_idx], labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([64, 91])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,  7758,  1750, 19304],\n",
      "        [    0,     0,     0,  ..., 10960,  5172,  6663],\n",
      "        [    0,     0,     0,  ...,  6091,  3135, 21049],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ...,  5611, 15878, 15048],\n",
      "        [    0,     0,     0,  ..., 20384,  7678,  9270],\n",
      "        [    0,     0,     0,  ...,  8381,  6820,  5578]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([64])\n",
      "Sample label: \n",
      " tensor([1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 2, 1, 3, 1, 1, 1, 1, 1, 1, 1, 2,\n",
      "        1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 3, 4, 2, 1, 1, 2, 2, 3, 3, 1, 2, 1, 1,\n",
      "        1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HateSpeechClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, cnn_params, pool_params,\n",
    "                 hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        TO BE RESTATED\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them\n",
    "        :param cnn_params: A 4-element tuple containing the number \n",
    "            of feature maps, kernel size, stride and padding of a Conv1D layer. \n",
    "        :param pool_params: A 3-element tuple containing the kernel size, stride and padding of a MaxPool1D layer. \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        super(HateSpeechClassifier, self).__init__()\n",
    "       \n",
    "        # set class variables\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # define model layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.conv = nn.Conv1d(embedding_dim, *cnn_params)\n",
    "        \n",
    "        self.pool = nn.MaxPool1d(*pool_params)\n",
    "        \n",
    "        n_maps, _, _, _ = cnn_params\n",
    "        self.lstm = nn.LSTM(n_maps, hidden_dim, n_layers, \n",
    "                            dropout=dropout, batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "    \n",
    "    \n",
    "    def forward(self, nn_input, hidden, test_print=False):\n",
    "        \"\"\"\n",
    "        Forward propagation of the neural network\n",
    "        :param nn_input: The input to the neural network\n",
    "        :param hidden: The hidden state        \n",
    "        :return: Two Tensors, the output of the neural network and the latest hidden state\n",
    "        \"\"\"\n",
    "        # TODO: Implement function   \n",
    "        batch_size = nn_input.size(0)\n",
    "\n",
    "        # embeddings\n",
    "        nn_input = nn_input.long()\n",
    "        embeds = self.embedding(nn_input)\n",
    "        \n",
    "        # Change axes. embedding_dim (in_channels) should be in the middle\n",
    "        # [batch_size, seq_length, embedding_dim] -> [batch_size, embedding_dim, seq_length]\n",
    "        embeds_t = embeds.permute(0, 2, 1)\n",
    "        \n",
    "        # conv\n",
    "        conv_out = self.conv(embeds_t)\n",
    "        \n",
    "        # pool\n",
    "        pool_out = self.pool(F.relu(conv_out))\n",
    "        \n",
    "        # Change axes. lstm expects features to be the last channel\n",
    "        # [batch_size, n_maps, down_sampled_seq] -> [batch_size, down_sampled_seq, n_maps]\n",
    "        pool_out_t = pool_out.permute(0, 2, 1)\n",
    "        \n",
    "        # lstm\n",
    "        lstm_out, hidden = self.lstm(pool_out_t, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        # out = self.dropout(lstm_out)\n",
    "        fc_out = self.fc(lstm_out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        fc_out_t = fc_out.view(batch_size, -1, self.output_size)  \n",
    "        \n",
    "        out = fc_out_t[:, -1] # get last batch of labels\n",
    "        \n",
    "        if test_print:\n",
    "            print(\"nn_input.\\nexpected : [batch_size, seq_length].\\nshape: {}\\n\".format(nn_input.shape))\n",
    "            print(\"embeds.\\nexpected : [batch_size, seq_length, embedding_dim].\\nshape: {}\\n\".format(embeds.shape))\n",
    "            print(\"embeds_t.\\nexpected : [batch_size, embedding_dim, seq_length].\\nshape: {}\\n\".format(embeds_t.shape))\n",
    "            print(\"conv_out.\\nexpected : [batch_size, n_maps, seq_length].\\nshape: {}\\n\".format(conv_out.shape))\n",
    "            print(\"pool_out.\\nexpected : [batch_size, n_maps, down_sampled_seq].\\nshape: {}\\n\".format(pool_out.shape))\n",
    "            print(\"pool_out_t.\\nexpected : [batch_size, down_sampled_seq, n_maps].\\nshape: {}\\n\".format(pool_out_t.shape))\n",
    "            print(\"lstm_out.\\nexpected : [batch_size, down_sampled_seq, hidden_dim].\\nshape: {}\\n\".format(lstm_out.shape))\n",
    "            print(\"lstm_out.\\nexpected : [batch_size * down_sampled_seq, hidden_dim].\\nshape: {}\\n\".format(lstm_out.shape))\n",
    "            print(\"fc_out.\\nexpected : [batch_size * down_sampled_seq, output_dim].\\nshape: {}\\n\".format(fc_out.shape))\n",
    "            print(\"fc_out_t.\\nexpected : [batch_size, down_sampled_seq, output_dim].\\nshape: {}\\n\".format(fc_out_t.shape))\n",
    "            print(\"out.\\nexpected : [batch_size, output_dim].\\nshape: {}\\n\".format(out.shape))\n",
    "                  \n",
    "        # return one batch of output word scores and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        '''\n",
    "        Initialize the hidden state of an LSTM/GRU\n",
    "        :param batch_size: The batch_size of the hidden state\n",
    "        :return: hidden state of dims (n_layers, batch_size, hidden_dim)\n",
    "        '''\n",
    "        # Implement function\n",
    "        \n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "def _test_HateSpeechClassifier(test_print):\n",
    "    batch_size = 20\n",
    "    sequence_length = 14\n",
    "    vocab_size = 25\n",
    "    output_size= 4\n",
    "    embedding_dim= 16\n",
    "    hidden_dim = 12\n",
    "    n_layers = 2\n",
    "    cnn_params = (5, 3, 1, 1)\n",
    "    pool_params = (2, 2, 0)\n",
    "    \n",
    "    # Initialize model\n",
    "    test_classifier = HateSpeechClassifier(vocab_size, output_size, embedding_dim, \n",
    "                                           cnn_params, pool_params, hidden_dim, n_layers)\n",
    "    \n",
    "    # create test input\n",
    "    X_npy = np.random.randint(vocab_size, size=(batch_size, sequence_length))\n",
    "    X = torch.from_numpy(X_npy)\n",
    "    \n",
    "    # Move to GPU if available\n",
    "    if(train_on_gpu):\n",
    "        test_classifier.cuda()\n",
    "        X = X.cuda()\n",
    "    \n",
    "    # Compute\n",
    "    hidden = test_classifier.init_hidden(batch_size)\n",
    "    out, hidden_out = test_classifier(X, hidden)\n",
    "    \n",
    "    # Test output and hidden state shapes\n",
    "    assert out.shape == (batch_size, output_size)\n",
    "    assert hidden_out[0].size() == (n_layers, batch_size, hidden_dim)\n",
    "    \n",
    "_test_HateSpeechClassifier(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
