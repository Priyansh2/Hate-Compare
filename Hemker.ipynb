{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Replication of Hemker (2018)\n",
    "\n",
    "The goal of this notebook is to follow the methodology explained in Hemker (2018) to perform a replication of his results. Note that the source code is not available, rendering this task a bit harder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: Davidson et al. (2017)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"./data/labeled_data.csv\", index_col=0)\n",
    "raw_tweets = df.tweet\n",
    "raw_labels = df[\"class\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>hate_speech</th>\n",
       "      <th>offensive_language</th>\n",
       "      <th>neither</th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   count  hate_speech  offensive_language  neither  class  \\\n",
       "0      3            0                   0        3      2   \n",
       "1      3            0                   3        0      1   \n",
       "2      3            0                   3        0      1   \n",
       "3      3            0                   2        1      1   \n",
       "4      6            0                   6        0      1   \n",
       "\n",
       "                                               tweet  \n",
       "0  !!! RT @mayasolovely: As a woman you shouldn't...  \n",
       "1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...  \n",
       "2  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...  \n",
       "3  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...  \n",
       "4  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...  "
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "---\n",
    "\n",
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a raw tweet:\n",
      "\"@Almightywayne__: @JetsAndASwisher @Gook____ bitch fuck u http://t.co/pXmGA68NC1\" maybe you'll get better. Just http://t.co/TPreVwfq0S\n",
      "\n",
      "Its cleaned version is:\n",
      " ||Quotation_Mark|| MENTIONHERE : MENTIONHERE MENTIONHERE bitch fuck u URLHERE ||Quotation_Mark|| maybe you'll get better ||Period|| just URLHERE \n"
     ]
    }
   ],
   "source": [
    "# Source: Davidson et al. (2017)\n",
    "\n",
    "import re\n",
    "import html\n",
    "from string import punctuation\n",
    "\n",
    "def preprocess(text_string):\n",
    "    \n",
    "    # Casing should not make a difference in our case\n",
    "    text_string = text_string.lower()\n",
    "    \n",
    "    # Regex\n",
    "    html_pattern = r'(&(?:\\#(?:(?:[0-9]+)|[Xx](?:[0-9A-Fa-f]+))|(?:[A-Za-z0-9]+));)'    \n",
    "    space_pattern = '\\s+'\n",
    "    giant_url_regex = ('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|'\n",
    "        '[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    mention_regex = '@[\\w\\-]+'\n",
    "    hashtag_regex = '#[\\w\\-]+'\n",
    "    \n",
    "    # First, add space surrounding HTML entities\n",
    "    text_string = re.sub(html_pattern, r' \\1 ', text_string)\n",
    "    \n",
    "    # Now, if we wish to find hashtags, we have to unescape HTML entities\n",
    "    text_string = html.unescape(text_string)\n",
    "    \n",
    "    # From Udacity TV script generation project\n",
    "    # Replace some punctuation by dedicated tokens\n",
    "    symbol_to_token = {\n",
    "        '.' : '||Period||',\n",
    "        ',' : '||Comma||',\n",
    "        '\"' : '||Quotation_Mark||',\n",
    "        ';' : '||Semicolon||',\n",
    "        '!' : '||Exclamation_Mark||',\n",
    "        '?' : '||Question_Mark||',\n",
    "        '(' : '||Left_Parenthesis||',\n",
    "        ')' : '||Right_Parenthesis||',\n",
    "        '-' : '||Dash||',\n",
    "        '\\n' : '||Return||'\n",
    "    }\n",
    "    \n",
    "    # Next, find URLs\n",
    "    text_string = re.sub(giant_url_regex, ' URLHERE ', text_string)\n",
    "    \n",
    "    # Then, tokenize punctuation\n",
    "    for key, token in symbol_to_token.items():\n",
    "        text_string = text_string.replace(key, ' {} '.format(token))\n",
    "\n",
    "    # Finally, remove spaces and find mentions and hashtags\n",
    "    text_string = re.sub(hashtag_regex, ' HASHTAGHERE ', text_string)\n",
    "    text_string = re.sub(mention_regex, ' MENTIONHERE ', text_string)\n",
    "    text_string = re.sub(space_pattern, ' ', text_string)\n",
    "    \n",
    "    return text_string\n",
    "\n",
    "def _test_preprocess():\n",
    "    \n",
    "    assert \" HASHTAGHERE \" == preprocess(\"#iam1hashtag\")\n",
    "    assert \" URLHERE \" == preprocess(\"https://seminar.minerva.kgi.edu\")\n",
    "    assert \" MENTIONHERE \" == preprocess(\"@vinimiranda\")\n",
    "    assert ' ' == preprocess(\"        \")\n",
    "    assert \" & MENTIONHERE URLHERE HASHTAGHERE \" == \\\n",
    "        preprocess(\"&amp;@vinimiranda    https://seminar.minerva.kgi.edu     #minerva    \")\n",
    "    \n",
    "_test_preprocess()\n",
    "\n",
    "print(\"Example of a raw tweet:\\n{}\".format(raw_tweets[68]))\n",
    "print(\"\\nIts cleaned version is:\\n{}\".format(preprocess(raw_tweets[68])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = raw_tweets.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.329, 'neu': 0.541, 'pos': 0.131, 'compound': -0.6597}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as VS\n",
    "\n",
    "sentiment_analyzer = VS()\n",
    "\n",
    "# Example\n",
    "sentiment_analyzer.polarity_scores(tweets[68])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lookup Tables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Udacity script project\n",
    "\n",
    "SPECIAL_WORDS = {'PADDING': '<PAD>'}\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: Tweets\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Generate vocabulary\n",
    "    vocab = set()\n",
    "    tweets.str.split().apply(vocab.update)\n",
    "    vocab.update(set(SPECIAL_WORDS.values()))\n",
    "    \n",
    "    # Generate lookup tables\n",
    "    vocab_to_int = {word : ii for ii, word in enumerate(vocab)}    \n",
    "    int_to_vocab = {ii : word for word, ii in vocab_to_int.items()}\n",
    "    \n",
    "    # return tuple\n",
    "    return (vocab_to_int, int_to_vocab)\n",
    "\n",
    "def _test_lookup_tables():\n",
    "    \n",
    "    text = np.array([\"this is a toy\", \"I mean not really a toy\", \"I mean a toy vocabulary\"])\n",
    "    vocab_to_int, int_to_vocab = create_lookup_tables(text)\n",
    "    \n",
    "    # Make sure the dicts make the same lookup\n",
    "    missmatches = [(word, id, id, int_to_vocab[id]) for word, id in vocab_to_int.items() if int_to_vocab[id] != word]\n",
    "    \n",
    "    assert not missmatches,\\\n",
    "        'Found {} missmatche(s). First missmatch: vocab_to_int[{}] = {} and int_to_vocab[{}] = {}'.format(len(missmatches),\n",
    "                                                                                                          *missmatches[0])\n",
    "    \n",
    "_test_lookup_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int, int_to_vocab = create_lookup_tables(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the vocabulary is: 21134 tokens.\n",
      "These are 10 randomly sample words in the vocabulary:\n",
      "['manuals', 'shovel', 'bus', 'kaffirs', 'evidence', 'zoned', 'eggs', 'tracks', 'detailed', 'boneless']\n"
     ]
    }
   ],
   "source": [
    "print(\"The size of the vocabulary is: {} tokens.\".format(len(vocab_to_int)))\n",
    "vocab = list(vocab_to_int.keys())\n",
    "np.random.shuffle(vocab)\n",
    "print(\"These are 10 randomly sample words in the vocabulary:\\n{}\".format(vocab[:10]))\n",
    "del vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hate Subclass Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partly from https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list\n",
    "# I do not implement co-reference resolution since a single NE is sufficient for directed hate speech.\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "\n",
    "def hate_classification(hate_tweet):\n",
    "    '''Receives a hateful tweet. \n",
    "       Return 3 for directed hate speech and 4 otherwise.'''\n",
    "    \n",
    "    if bool(hate_tweet.count(\"MENTIONHERE\")): return(3)\n",
    "    \n",
    "    # Remove tokens since they will oncused the POS tagger\n",
    "    token_regex = '\\|\\|\\w+\\|\\|'\n",
    "    hate_tweet = re.sub(token_regex, \"\", hate_tweet)\n",
    "    \n",
    "    # URLHERE is considered a proper noun by the pos tagger.\n",
    "    # Remove them before checking for proper nouns\n",
    "    no_punct_hate = ''.join([char for char in hate_tweet if char not in punctuation])\n",
    "    no_URL_hate = ' '.join([token for token in no_punct_hate.split() if token != \"URLHERE\"])\n",
    "    has_NE = False\n",
    "    for sent in nltk.sent_tokenize(no_URL_hate):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                return(3)  # Named Entity found    \n",
    "\n",
    "    return(4)\n",
    "        \n",
    "def _test_hate_classification():\n",
    "    assert hate_classification(\"MENTIONHERE\") == 3\n",
    "    assert hate_classification(\"Karen is absolutely crazy\") == 3\n",
    "    assert hate_classification(\"Karen is his sister. She's absolutely crazy\") == 3\n",
    "    assert hate_classification(\"They should all be sent to Mexico\") == 3\n",
    "    assert hate_classification(\"They should all leave the country\") == 4\n",
    "    assert hate_classification(\"some hate speech stuff\") == 4\n",
    "    assert hate_classification(\"\") == 4\n",
    "\n",
    "_test_hate_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a hateful tweet: \n",
      " ||Quotation_Mark|| we're out here ||Comma|| and we're queer ||Exclamation_Mark|| ||Quotation_Mark|| ||Return|| ||Quotation_Mark|| 2 ||Comma|| 4 ||Comma|| 6 ||Comma|| hut ||Exclamation_Mark|| we like it in our butt ||Exclamation_Mark|| ||Quotation_Mark|| \n",
      "Its type of hate speech is: Generalized\n",
      "\n",
      "Example of a hateful tweet:\n",
      " ||Quotation_Mark|| MENTIONHERE: jackies a retard HASHTAGHERE ||Quotation_Mark|| at least i can make a grilled cheese ||Exclamation_Mark|| \n",
      "Its type of hate speech is: Directed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hate_tweets = tweets[df[\"class\"] == 0].values\n",
    "_hate_prnt = lambda x : \"Generalized\" if hate_classification(x) == 4 else \"Directed\"\n",
    "\n",
    "print(\"Example of a hateful tweet: \\n{}\".format(hate_tweets[20]))\n",
    "print(\"Its type of hate speech is: {}\\n\".format(_hate_prnt(hate_tweets[20])))\n",
    "\n",
    "print(\"Example of a hateful tweet:\\n{}\".format(hate_tweets[10]))\n",
    "print(\"Its type of hate speech is: {}\\n\".format(_hate_prnt(hate_tweets[10])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change hate speech labels (0) to directed (3) / generalized labels (4) \n",
    "labels = raw_labels.copy()\n",
    "for i, (tweet, label) in enumerate(zip(tweets, raw_labels)):\n",
    "    \n",
    "    if label == 0:  # If hate speech\n",
    "        labels[i] = hate_classification(tweet)\n",
    "\n",
    "def _test_labels():\n",
    "    assert 1 not in pd.Series(labels).value_counts().index\n",
    "    assert 3 in pd.Series(labels).value_counts().index\n",
    "    assert 4 in pd.Series(labels).value_counts().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    19190\n",
       "2     4163\n",
       "3     1183\n",
       "4      247\n",
       "dtype: int64"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "---\n",
    "### Check Access to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU found. Please use a GPU to train your neural network.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check for a GPU\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if not train_on_gpu:\n",
    "    print('No GPU found. Please use a GPU to train your neural network.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:capstone]",
   "language": "python",
   "name": "conda-env-capstone-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
